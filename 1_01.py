import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample

# === 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===

# –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
binary_features = [...]    # —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Å 0/1
numeric_features = [...]   # —Å–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
target = 'y'

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
X_bin = df[binary_features]
X_num = df[numeric_features]
y = df[target]

# –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
scaler = StandardScaler()
X_num_scaled = scaler.fit_transform(X_num)

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
X_full = pd.concat([X_bin.reset_index(drop=True), 
                    pd.DataFrame(X_num_scaled, columns=numeric_features)], axis=1)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.3, random_state=42, stratify=y)

# === 2. –ú–æ–¥–µ–ª—å 1: LogisticRegression c class_weight='balanced' ===

model1 = LogisticRegression(solver='saga', penalty='l2', class_weight='balanced', max_iter=1000)
model1.fit(X_train, y_train)
y_pred1 = model1.predict(X_test)
y_proba1 = model1.predict_proba(X_test)[:, 1]

# === 3. –ú–æ–¥–µ–ª—å 2: Oversampling + –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ç–µ—Ä—Å–µ–ø—Ç–∞ ===

# –°–æ–±–∏—Ä–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è oversampling
train_df = X_train.copy()
train_df['y'] = y_train.values

df_majority = train_df[train_df.y == 0]
df_minority = train_df[train_df.y == 1]

# Oversample –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –¥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ majority
df_minority_over = resample(df_minority, 
                            replace=True, 
                            n_samples=len(df_majority), 
                            random_state=42)

df_balanced = pd.concat([df_majority, df_minority_over])
X_train_bal = df_balanced.drop(columns='y')
y_train_bal = df_balanced['y']

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
model2 = LogisticRegression(solver='saga', penalty='l2', max_iter=1000)
model2.fit(X_train_bal, y_train_bal)

# –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ç–µ—Ä—Å–µ–ø—Ç–∞ –ø–æ —Ñ–æ—Ä–º—É–ª–µ Manski & Lerman
p = y_train.mean()   # —Ä–µ–∞–ª—å–Ω–∞—è –¥–æ–ª—è –∫–ª–∞—Å—Å–∞ 1 –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
r = 0.5              # –≤ oversampled –≤—ã–±–æ—Ä–∫–µ –∫–ª–∞—Å—Å 1 = 50%
b0_raw = model2.intercept_[0]
correction = np.log((p / r) * ((1 - r) / (1 - p)))
model2.intercept_ = np.array([b0_raw + correction])

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred2 = model2.predict(X_test)
y_proba2 = model2.predict_proba(X_test)[:, 1]

# === 4. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π ===

def evaluate_model(y_true, y_pred, y_proba, name):
    print(f"\nüìä {name}")
    print(classification_report(y_true, y_pred, digits=3))
    print(f"AUC ROC: {roc_auc_score(y_true, y_proba):.4f}")

evaluate_model(y_test, y_pred1, y_proba1, "–ú–æ–¥–µ–ª—å 1: class_weight='balanced'")
evaluate_model(y_test, y_pred2, y_proba2, "–ú–æ–¥–µ–ª—å 2: oversampling + –∏–Ω—Ç–µ—Ä—Å–µ–ø—Ç")
